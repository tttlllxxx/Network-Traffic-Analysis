## N维数组样例
N维数组是机器学习和神经网络主要数据结构

0-d（标量）
`1.0`
一个类别

1-d（向量）
`[1.0,2.7,3.4]`
一个特征向量

2-d（矩阵）
```python
[[1.0,2.7,3.4]
 [5.0,0.2,4.6]
 [4.3,8.5,0.2]]
```
一个样本-特征矩阵

3-d
RGB图片（宽×高×通道）
```python
[[[1.0,2.7,3.4]
  [5.0,0.2,4.6]
  [4.3,8.5,0.2]]
 [[1.0,2.7,3.4]
  [5.0,0.2,4.6]
  [4.3,8.5,0.2]]]
```

4-d
RGB图片批量（批量大小×宽×高×通道）

5-d
一个视频批量（批量大小×时间×宽×高×通道）

## 创建数组
- 创建数组需要
	- 形状：例如3×4矩阵
	- 每个元素的数据类型
	- 每个元素的值

## 访问元素
一个元素：`[1,2]`
一行：`[1,:]`
一列：`[:,1]`
子区域：`[1:3,1:]`   `1:3`(3是开区间)  表示第1行到第二行，`1:`表示第一列以及后面所有
子区域：`[::3,::2]`  `::3`每3行一跳，`::2`每2列一条
（张量操作中都是左闭右开）
## 数据操作
首先导入torch
```python
import torch
```

张量表示一个数值组成的数组，数组可能有多个维度
```python
x=torch.arrange(12)
x
```
```Out
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```

可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的形状
`numel`属性访问张量中的元素总数
```python
x.shape
```
```Out
torch.Size([12])
```

```python
x.numel()
```
```Out
12
```

要改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数
```Python
X = x.reshape(3, 4)
X
```
```Out
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
```
可以通过`-1`来调用此自动计算出维度的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。

使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。
```Python
torch.zeros((2, 3, 4))
torch.ones((2, 3, 4))
torch.randn(3, 4)
```

还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。
```Python
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
```

## 运算符
对于任意具有相同形状的张量， 常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按**元素**运算。 我们可以在同一形状的任意两个张量上调用按元素操作。包括像求幂这样的一元运算符.
```Python
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算
torch.exp(x)
```
```Out
(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))
 tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
```
也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 
下面的例子分别演示了当我们沿行（轴0（dim=0），形状的第一个元素） 和按列（轴1（dim=1），形状的第二个元素）连结两个矩阵时。
```Python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```
```Out
(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
```
通过逻辑运算符构建二元张量。
```Python
X == Y
```
```Out
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
```
对张量中的所有元素进行求和，会产生一个单元素张量。
```Python
X.sum()
```
```Out
tensor(66.)
```

## 广播机制
在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。 在某些情况下，即使形状不同，我们仍然可以通过调用 _广播机制_（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：
1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：
```Python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```
```Out
(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))
```
由于`a`和`b`分别是`3×1`和`1×2`矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵**广播**为一个更大的矩阵，如下所示：矩阵`a`将复制列， 矩阵`b`将复制行，然后再按元素相加。
```Python
a + b
```
```Out
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

## 索引和切片
张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。
```Python
X[-1], X[1:3]
```
```Out
(tensor([ 8.,  9., 10., 11.]),
 tensor([[ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]))
```
除读取外，我们还可以通过指定索引来将元素写入矩阵。
```Python
X[1, 2] = 9
X
```
```Out
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  9.,  7.],
        [ 8.,  9., 10., 11.]])
```
如果想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。
```Python
X[0:2, :] = 12
X
```
```Out
tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
```

## 节省内存
运行一些操作可能会导致为新结果分配内存。 例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。
用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。
```Python
before = id(Y)
Y = Y + X
id(Y) == before
```
```Out
False
```
这可能是不可取的，原因有两个：
1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全的块。
```Python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
```
```Out
id(Z): 1957717877888
id(Z): 1957717877888
```
`Z[:] = X + Y` 表示 **把 `X+Y` 的结果拷贝到 `Z` 的内存里**，并没有改变 `Z` 本身的对象，只是修改了它的内容。而如果直接`Z = X + Y`，`X + Y` 会新建一个 **新的张量对象**，把它赋值给变量 `Z`，这样 `Z` 不再指向原来的 `torch.zeros_like(Y)` 的那块内存，而是换成了一个新对象。
- **`Z[:] = ...`**：保持 `Z` 对象不变，只修改其内部数据（in-place 修改）。
- **`Z = ...`**：让 `Z` 指向一个新的张量对象（out-of-place 赋值）。
如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。

## 转换为其他Python对象
将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。
```Python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```
```Out
(numpy.ndarray, torch.Tensor)
```
要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。
```Python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```
```Out
(tensor([3.5000]), 3.5, 3.5, 3)
```