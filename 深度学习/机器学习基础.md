
**机器学习（Machine Learning, ML）**：
机器具备学习能力，让机器具备找一个函数的功能。实现任务需要复杂函数，通过机器的力量把函数自动找出来。
类别：
- **回归**：函数输出是一个数值。
- **分类**：函数输出是在设定好的选项里选择一个。
- **结构化学习**：函数输出一个有结构的物体，比如一张图，一篇文章。 
  
1.  建立模型（model，带有未知参数的函数）$$y=b+wx_1$$
	$x_1$特征(feature)，$w$权重(weight)，$b$偏置(bias)。

2. 定义损失（loss）
   损失也是一个函数。输入是模型里的参数$b$和$w$，即$L(b,w)$。输出值表示把这一组未知参数设置为某个值时，这笔数值是好还是不好。
   估测值$\hat{y}$与真实值$y$（标签，label）比较。
   
   >[!note]    损失函数
   >- 平均绝对误差（Mean Absolute Error，MAE）
 >$$e=|\hat{y}-y|$$
   >- 均方误差（Mean Squared Error，MSE）
> $$e=(\hat{y}-y)^2$$
   >- 交叉熵（cross entropy） 
   
3. 解一个最优化的问题
  找一个 $w$ 跟 $b$，把未知的参数找一个数值出来，看代哪一个数值进去可以让损失$L$的值最小。这个可以让损失最小的 $w$ 跟 $b$ 称为$w^∗$ 跟 $b^∗$ 代表它们是最好的一组 $w$ 跟 $b$ ，可以让损失的值最小。
  
  梯度下降（gradient descent）
  先假设只有一个未知参数$w$，$b$已知。$w$代入不同数值，得到不同损失，所得的曲线就是误差表面。随机选取一个初始点$w^0$，计算$\left. \frac{\partial L}{\partial w} \right|_{w = w^0}$，即在$w=w^0$时损失关于参数$w$的偏导数。
  计算在这一个点，在 $w^0$这个位置的误差表面的切线斜率，也就是这一条蓝色的虚线，
   如果斜率是负的，就代表右边比较低，$w$往右边移，把$w$变大可以让损失变小。
   如果斜率是正的，就代表左边比较低，$w$往左边移，把$w$变小可以让损失变小。
  无论 $\frac{\partial L}{\partial w}$ 是正是负，始终朝着损失下降的方向移动。
  
  移动大小取决于：
  - 斜率
  - 学习率（learning rate）：学习率是自己设定的，如果 $η$ 设大一点，每次参数更新量就会大，学习可能就比较快。如果 $η$ 设小一点，参数更新就很慢，每次只会改变一点点参数的数值。

  >[!note]  需要自己设定，不是机器自己找出来的，称为超参数（hyperparameter）。

把$w^0$往右移一步，新的位置为$w^1$，这一步的步伐是 $η$ 乘上微分的结果，即：
$$w^1\gets w^0-η\left. \frac{\partial L}{\partial w} \right|_{w = w^0}$$
![[优化过程.png]]

反复进行刚才的操作，计算一下$w^1$ 微分的结果，再决定现在要把$w^1$移动多少，再移动到$w^2$，再继续反复做同样的操作，不断地移动$w$的位置，最后会停下来。往往有两种情况会停下来。

 - 第一种情况是一开始会设定，在调整参数计算微分的时候，最多计算几次。上限可能会设为 100 万次，参数更新 100 万次后，就不再更新了，更新次数也是一个超参数。
   
- 还有另外一种理想情况，当不断调整参数，调整到一个地方，它的微分的值就是这一项，算出来正好是 0 的时候，0 乘上学习率 $η$ 还是 0，所以参数就不会再移动位置。假设是这个理想的情况，把$w^0$ 更新到 $w^1$，再更新到 $w^2$，最后更新到 $w^T$时算出来这个微分的值是0了，参数的位置就不会再更新。
  
梯度下降有一个很大的问题，没有找到真正最好的解，没有找到可以让损失最小的$w$。
![[局部最小值.png]]
推广到两个参数$w$和$b$：
$$\left. \frac{\partial L}{\partial w} \right|_{w = w^0，b=b^0}$$
$$\left. \frac{\partial L}{\partial b} \right|_{w = w^0，b=b^0}$$$$w^1\gets w^0-η\left. \frac{\partial L}{\partial w} \right|_{w = w^0，b=b^0}$$$$b^1\gets b^0-η\left. \frac{\partial L}{\partial b} \right|_{w = w^0，b=b^0}$$


**深度学习（Deep Learing，DL）**：