## 代码
```Python
class MinimindModel(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.vocab_size, self.num_hidden_layer = (
            config.vocab_size, # 词表大小
            config.num_hidden_layers, # Transform block层数
        )

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)

        self.dropout = nn.Dropout(config.dropout)

        self.layers = nn.ModuleList(
            [MiniMindBlock(i, config) for i in range(self.num_hidden_layer)]
        )

        # RoPe预计算
        freqs_cos, freqs_sin = precomput_freqs_cis(
            dim=config.hidden_size//config.num_attention_heads,
            end=config.max_position_embeddings,
            rope_base=config.rope_theta,
            rope_scaling=config.rope_scaling,
        )

        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)
        
    def forward(
        self,
        input_ids:Optional[torch.Tensor]=None,
        attention_mask:Optional[torch.Tensor]=None,
        past_key_value:Optional[Tuple[Tuple[torch.Tensor]]]=None,
        use_cache:bool=False,
        **kwargs,
    ):
        batch_size,seq_len=input_ids.shape
        
        if hasattr(past_key_value,'layers'):
            past_key_value=None
            
        past_key_value=past_key_value or [None]*len(self.layers)
        
        start_pos=( # 起始位置
            past_key_value[0][0].shape[1] if past_key_value[0] is not None else 0
        )
        
        hidden_states=self.dropout(self.embed_tokens(input_ids))
        
        position_embeddings=(
            self.freqs_cos[start_pos:start_pos+seq_len],
            self.freqs_sin[start_pos:start_pos+seq_len],
        )
        
        present=[]
        
        for layer_idx,(layer,past_key_value) in enumerate(
            zip(self.layers,past_key_value)
        ):
            hidden_states,present=layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask,
            )
            
            present.append(present)
            
        hidden_states=self.norm(hidden_states)
        
        return hidden_states.presents
```