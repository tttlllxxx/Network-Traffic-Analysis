**MHA**：一个Q对应一个KV
**MQA**：所有Q共享同一个KV
**GQA**：多个Q共享一组K和V

4个Q共享一个K和V效果最好

`d_model` 总维度
`num_heads` Q的头数（通常为8）
`num_key_value_heads` K,V的头数
$$head\_dim=\frac{d\_model}{num\_heads}$$

$$\frac{num\_key\_value\_heads}{num\_heads}=n\_rep=4$$


> [! 为什么不采用比如多个K共享一个]
>  QKV可以看作是数据库中的query（询问），key，value。kv并不重要，如何query才重要。
>  每次都是拿当前词Q去和所有之前的KV计算。
>  Q是提问者，是融合上下文获取不同词意的核心，KV负责提供信息。

## ResNet（残差网络）
假设某个完美的function是$H(x)=x$
没有残差网络 $H(x)=F(x)$就要费力的拟合$x$
有残差网络 $H(x)=F(x)+x$只需要$F(x)=0$
残差在梯度上的好处：
$$\frac{dL}{dx}=\frac{dL}{dH}\times\frac{dH}{dx}=\frac{dL}{dH}\cdot(\frac{dF}{dx}+1)$$
$+1$可以有效缓解梯度消失问题
## 相关torch方法

```Python
import torch
import torch.nn as nn
```

### nn.Dropout
```Python
# 防止过拟合，p为Dropout的概率
dropout_layer=nn.Dropout(p=0.5)

t1=torch.Tensor([1,2,3])
t2=dropout_layer(t1)
# 这里Dropout丢弃为了保持期望不变，将其他部分扩大2（1/1-p）倍
print(t2) # Output:tensor([2.,4.,0.])
```

### nn.Linear
$$y=xW^T+b$$
```Python
layer=nn.Linear(in_features=3,out_features=5,bias=True) 
# in_features：输入特征维度；out_features：输出特征维度；bias：是否使用偏置
# W的shape(out_features,in_features)；b的shape(out_features,)
t1=torch.Tensor([1,2,3]) # shape:(3,)

t2=torch.Tensor([[1,2,3]]) # shape:(1,3)
# 这里应用的w和b随机，真实训练会在optimizer上更新
output1=layer(t1) #shape:(5,)
output2=layer(t2) #shape:(1,5)
print(output) # Output:tensor([[-1.5081,  0.3979,  2.3418,  1.9570, -0.0834]]
# 线性变换，对应用的张量乘以一个w矩阵然后+b
```

### torch.view
```Python
t=torch.tensor([1,2,3,4,5,6],[7,8,9,10,11,12]) # [2,6]
t_view1=t.view(3,4)
print(t_view1)
# tensor([[1,2,3,4],
#         [5,6,7,8],
#         [9,10,11,12]])
t_view2=t.view(4,3)
print(t_view2)
# tensor([[1,2,3],
#         [4,5,6],
#         [7,8,9],
#         [10,11,12]])
```

### torch.transpose 
```Python
t1=torch.Tensor([[1,2,3],[4,5,6]]) # [2,3]
t1=t1.transpose(0,1) # 交换第一维和第二维
# -1:最后一个维度,-2:倒数第二个维度
print(t1)
# tensor([[1,4]
#         [2,5]
#         [3,6]])
```

### torch.triu
```Python
x=torch.tensor([[1,2,3],[4,5,6],[7,8,9]]) # [3,3]

print(torch.triu(x))
# tensor([[1,2,3],
#         [0,5,6],
#         [0,0,9]])
print(torch.triu(x,diagonal=1)) # diagonal默认是0，>0 时上移对角线
# tensor([[0,2,3],
#         [0,0,6],
#         [0,0,0]])
print(torch.triu(x,diagonal=-1)) # <0 时下移对角线
# tensor([[1,2,3],
#         [4,5,6],
#         [0,8,9]])
```

### torch.reshape
```Python
x=torch.arange(1,7) # tensor([1, 2, 3, 4, 5, 6])

y=torch.reshape(x,(2,3))
print(y)
# tensor([[1, 2, 3],
#         [4, 5, 6]])
z=torch.reshape(x,(3,-1)) # -1自动推断形状
print(z)
# tensor([[1, 2],
#         [3, 4],
#         [5, 6]])
```

## 代码
```Python
def repeat_kv(x:torch.Tensor,n_rep:int)->torch.Tensor:
    bs,slen,num_key_value_heads,head_dim=x.shape
    # bs:batch_size;
    # slen:序列长度;
    # num_key_value_heads:K/V 的 head 数（通常小于 Q（num_heads））;
    # head_dim:head 的维度
    if n_rep==1:
        return x
    
    return (
    x[:,:,:,None,:]
    .expand(bs,slen,num_key_value_heads,n_rep,head_dim)
    .reshape(bs,slen,num_key_value_heads*n_rep,head_dim)
    )
    
class Attention(nn.Module):
    def __init__(self,args:MiniMindConfig):
        super().__init__()
        
        self.num_key_value_heads=(
            args.num_key_value_heads 
            if args.num_key_value_heads is None 
            else args.nums_attention_heads
        )
        
        assert args.nums_attention_heads % self.num_key_value_heads == 0,
        "num_attention_heads must be disvisible by num_key_value_heads"
        
        self.n_local_heads=args.num_attention_head
        self.n_local_kv_heads=self.num_key_value_heads
        self.n_rep=self.n_local_heads//self.n_local_kv_heads
        self.head_dim=args.hidden_size//args.num_attention_head
        
        # QKV投影
        self.q_proj=nn.Linear(
            args.hidden_size,
            args.num_attention_head*self.head_dim, # Q的头数*一个头的维度
            bias=False
        ) 
        self.k_proj=nn.Linear(
            args.hidden_size,
            self.n_local_kv_heads*self.head_dim,
            bias=False
        )
        self.v_proj=nn.Linear(
            args.hidden_size,
            self.n_local_kv_heads*self.head_dim,
            bias=False
        )
        self.o_proj=nn.Linear( # output
            args.num_attention_head*self.head_dim,
            args.hidden_size,
            bias=False
        )
        
        self.attn_dropout=nn.Dropout(args.dropout)
        self.resid_dropout=nn.Dropout(args.dropout)
        self.dropout=args.dropout
        
        self.flash=hasattr(torch.nn.functional,'scaled_dot_product_attention') and args.flash_attention
        
    
    def forward(
        self,
        x:torch.Tensor,
        position_embdding:Tuple[torch.Tensor,torch.Tensor],
        past_key_value:Optional[Tuple[torch.Tensor,torch.Tensor]]=None,
        use_cache=False,
        attention_mask:Optional[torch.Tensor]=None, # 外部传进来的约束信息
    )->torch.Tensor:
    
    # 投影，计算qkv
        bsz,seq_len,_=x.shape
        xq,xk,xv=self.q_proj(x),self.k_proj(x),self.v_proj(x)
    # 把输入拆分成多个头，用view
        xq=xq.view(bsz,seq_len,self.n_local_heads,self.head_dim)
        xk=xk.view(bsz,seq_len,self.num_key_value_heads,self.head_dim)
        xv=xv.view(bsz,seq_len,self.num_key_value_heads,self.head_dim)
    # q和k，使用RoPE
        cos,sin=position_embdding
        xq,xk=apply_rotary_pos_emb(xq,xk,cos[:seq_len],sin[:seq_len])
    # 对于k和v，使用repeat（注意kv 缓存（cache））
        if past_key_value is not None:
            xk=torch.cat([past_key_value[0],xk],dim=1)
            xv=torch.cat([past_key_value[1],xv],dim=1)
        past_kv=(xk,xv) if use_cache else None
        
        xq,xk,xv=(
            xq.transpose(1,2),
            # [bsz,seq_len,n_local_heads,head_dim]->
            # [bsz,n_local_heads,seq_len,head_dim] head是批次，不参与计算
            repeat_kv(xk,self.n_rep).transpose(1,2),
            repeat_kv(xv,self.n_rep).transpose(1,2),
        )
	    # 进行attention计算，q@k^T/sqrt(d)
        if (
            self.flash
            and seq_len > 1
            and (
                attention_mask is None
                or torch.all(attention_mask == 1)
            )  # 只有在「没有padding干扰」时，才安全地使用FlashAttention
        ):
            # 在满足条件时，用 FlashAttention 加速；否则回退到普通 Attention。
            attn_mask = (
                None  # 完全不使用 mask
                if attention_mask is None
                else attention_mask
                .view(bsz, 1, 1, -1)
                .expand(bsz, self.n_local_heads, seq_len, -1)
                .bool()
            )
            output = F.scaled_dot_product_attention(
                xq,  # [bsz, n_heads, seq_len, head_dim]
                xk,
                xv,
                attn_mask=attn_mask,
                # 训练时->使用 dropout
                # 推理时->关闭 dropout
                dropout_p=self.dropout if self.training else 0.0,
                is_causal=True  # 强制使用因果掩码（下三角）
            )
        else:
            # scores->[bsz,n_heads,seq_len,seq_len]
            # scores[i,h,q,k] = 第h个头中，第q个token对第k个token 的相似度
            scores = (xq@xk.transpose(-2, -1))/math.sqrt(self.head_dim)
            # 当前位置i不能看到未来j>i的token
            # scores的第i行，描述的是第i个token去关注所有第j个token。
            scores = scores+torch.triu(
                torch.full(
                    (seq_len, seq_len),
                    float('-inf'),
                    device=scores.device
                ),
                diagonal=1,
            ).unsqueeze(0).unsqueeze(0)
            # [1,1,seq_len,seq_len]->[bsz,n_heads,seq_len,seq_len]
            # 最后拼接头，输出投影，返回
            if attention_mask is not None:
                # attention_mask.shape=[bsz,seq_len]
                # 1 = 有效 token
                # 0 = PAD
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                # [bsz,seq_len]->[bsz,1,1,seq_len] 这里的seq_len对应的是k_len
                # 对所有query，屏蔽同一批key

                extended_attention_mask = (1.0-extended_attention_mask)*-1e9
                scores = scores+extended_attention_mask  # PAD位置的score→-∞
                # scores: [bsz, n_heads, q_len, k_len]
                # mask:   [bsz, 1,      1,     k_len]
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv
            # scores: [bsz, n_heads, q_len, k_len]
            # xv:     [bsz, n_heads, k_len, head_dim]
            # output: [bsz, n_heads, q_len, head_dim]
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        # 在训练态 self-attention 中：q_len == seq_len
        # [bsz, n_heads, q_len, head_dim]->[bsz, seq_len, n_heads,  head_dim]
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv
```

