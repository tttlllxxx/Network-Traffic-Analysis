## Norm
归一化：平均值为0，标准差为1。
$$y=W\cdot x +b$$
反向传播：
$$\frac{\partial L}{\partial W}=\frac{\partial L}{\partial y}\times\frac{\partial y}{\partial W}=\frac{\partial L}{\partial y}\cdot x$$
$L$为损失loss

由此可见，梯度与$x$本身的值（即输入向量）有关
如果$x$过大或者过小，都容易梯度爆炸或消失
所以需要归一化，标准差变为1
使用RMSNorm，比传统Norm少了均值相关计算
$$y_i=\frac{x_i}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i)^2
+\epsilon}}\cdot\gamma$$
$\gamma$ 是用来缩放，如果没有则向量的长度将被固定，归一化后模型会无法恢复到原来的尺度。
$\gamma$ 让模型自己决定“这个特征该放大还是缩小”。
## 相关torch方法
### torch.rsqrt
```Python
# 开方求倒数
torch.rsqrt(torch.tensor(4.0))   # tensor(0.5)
```

### torch.ones
```Python
# 创建一个全1张量
torch.ones(3,4) 
```


## 代码
```Python
# 继承nn.Module类
class RMSNorm(nn.module):

# __init__初始化
    def __init__(self,dim:int,eps:float=1e-5):
        super().__init__()
        self.dim=dim
        self.eps=eps
        self.weight=nn.Parameter(torch.ones(dim)) # 定义一个可学习的缩放参数向量
# _norm
    def _norm(self,x):
	        return x*torch.sqrt(x.pow(2).mean(-1,keepdim=True)+self.eps)
# forward
    def forward(self,x):
        return self.weight*self._norm(x.float()).type_as(x)
```

`x.pow(2).mean(-1,keepdim=True)`就是$\frac{\sum_{i=1}^{n}(x_i)^2}{n}$
其中`mean(-1,keepdim=True)`沿着最后一个维度求均值，
如果`x`形状是`[batch, seq_len, dim]`，那`mean(-1)` 就得到`[batch, seq_len, 1]`。
