GQA和FFN通过残差连接(residual connection)和层归一化(LayerNorm)顺序地串联起来。
block循环，下一层的输入是上一层的hidden_states。
```
h₀ = embedding(x)
h₁ = Block₁(h₀)
h₂ = Block₂(h₁)
...
h_L = Block_L(h_{L-1})
```

## 代码
```Python
class MiniMindBlock(nn.Module):
    def __init__(self,layer_id:int,config:MiniMindConfig):
        super().__init__()
        self.num_attention_heads=config.num_attention_heads
        self.hidden_size=config.hidden_size
        self.head_dim=self.hidden_size//self.num_attention_heads
        self.self_attn=Attention(config)
        
        self.layer_id=layer_id
        self.input_layernorm=RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )
        self.post_attention_layernorm=RMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps
        )
        self.mlp=FeedForward(config)
    
    def forward(
        self,
        hidden_states, 
        # 输入的隐藏状态，通常是经过上一层处理后的特征表示（例如，来自前一层的输出）。
        position_embeddings,
        past_key_value=None,
        use_cache=False,
        attention_mask=None
    ):
        residual=hidden_states
        hidden_states,present_key_value=self.self_attn( # 自注意力机制GQA
            self.input_layernorm(hidden_states), # 来自上一层的隐藏状态
            position_embeddings, # 位置编码
            past_key_value,
            use_cache,
            attention_mask,
        ) # 自注意力计算
        hidden_states=hidden_states+residual # 残差连接
        hidden_states=hidden_states+self.mlp( # FFN
            self.post_attention_layernorm(hidden_states)
        )
        return hidden_states,present_key_value
```