## 什么是神经网络

神经网络本质是多层线性矩阵变换与非线性激活函数的组合。
$$y=σ(Wx+b)$$
- $x$：输入向量（例如图像的一部分）
- $W$：权重
- $b$：偏置
- $\sigma$：激活函数（ReLU、Sigmoid……）
- $y$：神经元输出

## 什么是Attention
Attention 让模型在处理一个输入时，自动学会“关注”其中最重要的部分。
Self-Attention 序列的每个位置都对序列的每个位置做 attention。Q、K、V 都来自同一序列。

| <center>名称</center> | <center>作用</center> | <center>类比</center> |
| ------------------- | ------------------- | ------------------- |
| Query               | 我在问：“我该关注谁？”        | 搜索关键词               |
| Key                 | 我在回答：“我是什么特征？”      | 文档标题                |
| Value               | 我提供的信息内容            | 文档正文                |
$$Attention(Q,K,V)=softmax\left(\frac{QK^T}{\sqrt{d}}\right)V$$

## 架构图
![[image.png]]